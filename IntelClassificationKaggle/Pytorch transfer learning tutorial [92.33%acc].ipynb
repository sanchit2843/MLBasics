{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install torchsummary\n!pip install efficientnet_pytorch\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn import metrics\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport cv2\nimport os\nimport torchvision\nimport shutil\nfrom torch.autograd import Variable\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torchsummary import summary\nfrom efficientnet_pytorch import EfficientNet\nimport torch.optim as optim\nfrom tqdm.autonotebook import tqdm\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset loading using torchvision module"},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters\nbatch_size = 32\nim_size = 150\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalization_parameter(dataloader):\n    mean = 0.\n    std = 0.\n    nb_samples = len(dataloader.dataset)\n    for data,_ in tqdm(dataloader):\n        batch_samples = data.size(0)\n        data = data.view(batch_samples, data.size(1), -1)\n        mean += data.mean(2).sum(0)\n        std += data.std(2).sum(0)\n    mean /= nb_samples\n    std /= nb_samples\n    return mean.numpy(),std.numpy()\nim_size = 150\ntrain_transforms = transforms.Compose([\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor()])\ntrain_data = torchvision.datasets.ImageFolder(root = '../input/seg_train/seg_train', transform = train_transforms)\ntrain_loader =  DataLoader(train_data, batch_size = batch_size , shuffle = True)\nmean,std = normalization_parameter(train_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#image transformations for train and test data\n\ntrain_transforms = transforms.Compose([\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.RandomResizedCrop(size=315, scale=(0.95, 1.0)),\n                                        transforms.RandomRotation(degrees=10),\n                                        transforms.RandomHorizontalFlip(),\n                                        transforms.CenterCrop(size=299),  # Image net standards\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\ntest_transforms = transforms.Compose([\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\n\n#inverse normalization for image plot\n\ninv_normalize =  transforms.Normalize(\n    mean=-1*np.divide(mean,std),\n    std=1/std\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_loader(train_data,test_data = None , valid_size = None , batch_size = 32):\n    train_loader =  DataLoader(train_data, batch_size = batch_size , shuffle = True)\n    if(test_data == None and valid_size == None):\n        print('c')\n        dataloaders = {'train':train_loader}\n        return dataloaders\n    if(test_data == None and valid_size!=None):\n        data_len = len(train_data)\n        indices = list(range(data_len))\n        np.random.shuffle(indices)\n        split1 = int(np.floor(valid_size * data_len))\n        valid_idx , test_idx = indices[:split1], indices[split1:]\n        valid_sampler = SubsetRandomSampler(valid_idx)\n        valid_loader = DataLoader(train_data, batch_size= batch_size, sampler=valid_sampler)\n        dataloaders = {'train':train_loader,'val':valid_loader}\n        return dataloaders\n    if(test_data != None and valid_size!=None):\n        data_len = len(test_data)\n        indices = list(range(data_len))\n        np.random.shuffle(indices)\n        split1 = int(np.floor(valid_size * data_len))\n        valid_idx , test_idx = indices[:split1], indices[split1:]\n        valid_sampler = SubsetRandomSampler(valid_idx)\n        test_sampler = SubsetRandomSampler(test_idx)\n        valid_loader = DataLoader(test_data, batch_size= batch_size, sampler=valid_sampler)\n        test_loader = DataLoader(test_data, batch_size= batch_size, sampler=test_sampler)\n        dataloaders = {'train':train_loader,'val':valid_loader,'test':test_loader}\n        return dataloaders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data loader\ntrain_data = torchvision.datasets.ImageFolder(root = '../input/seg_train/seg_train', transform = train_transforms)\ntest_data = torchvision.datasets.ImageFolder(root = '../input/seg_test/seg_test', transform = test_transforms)\ndataloaders = data_loader(train_data,test_data , valid_size = 0.2 , batch_size = batch_size)\n#label of classes\nclasses = train_data.classes\n#encoder and decoder to convert classes into integer\ndecoder = {}\nfor i in range(len(classes)):\n    decoder[classes[i]] = i\nencoder = {}\nfor i in range(len(classes)):\n    encoder[i] = classes[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n#plotting rondom images from dataset\ndef class_plot(data , encoder ,inv_normalize = None,n_figures = 12):\n    n_row = int(n_figures/4)\n    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=4)\n    for ax in axes.flatten():\n        a = random.randint(0,len(data))\n        (image,label) = data[a]\n        label = int(label)\n        l = encoder[label]\n        if(inv_normalize!=None):\n            image = inv_normalize(image)\n        \n        image = image.numpy().transpose(1,2,0)\n        im = ax.imshow(image)\n        ax.set_title(l)\n        ax.axis('off')\n    plt.show()\nclass_plot(train_data,encoder,inv_normalize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Declare model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using efficientnet model based transfer learning\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.resnet =  EfficientNet.from_pretrained('efficientnet-b3')\n        self.l1 = nn.Linear(1000 , 256)\n        self.dropout = nn.Dropout(0.75)\n        self.l2 = nn.Linear(256,6)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.resnet(input)\n        x = x.view(x.size(0),-1)\n        x = self.dropout(self.relu(self.l1(x)))\n        x = self.l2(x)\n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclassifier = Classifier().to(device)\n#uncomment to see summary of the created model\nsummary(classifier,(3,150,150))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss -> Negative log likelihood loss if output layer logsoftmax else for linear layer we use crossentropy loss.\ncriterion = nn.CrossEntropyLoss()\n#lr scheduler ->\n#learning rate half after 3 epochs\n# cyclical learning rate ->\n#Original learning rate restored after 10 epochs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/davidtvs/pytorch-lr-finder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copy('./pytorch-lr-finder/lr_finder.py','./lr_finder.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lr_finder import LRFinder\noptimizer_ft = optim.Adam(classifier.parameters(), lr=0.0000001)\nlr_finder = LRFinder(classifier, optimizer_ft, criterion, device=device)\nlr_finder.range_test(train_loader, end_lr=1, num_iter=500)\nlr_finder.reset()\nlr_finder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model,dataloaders,criterion,num_epochs=10,lr=0.00001,batch_size=8,patience = None):\n    since = time.time()\n    model.to(device)\n    best_acc = 0.0\n    i = 0\n    phase1 = dataloaders.keys()\n    losses = list()\n    acc = list()\n    if(patience!=None):\n        earlystop = EarlyStopping(patience = patience,verbose = True)\n    for epoch in range(num_epochs):\n        print('Epoch:',epoch)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        lr = lr*0.8\n        if(epoch%10==0):\n            lr = 0.0001\n\n        for phase in phase1:\n            if phase == ' train':\n                model.train()\n            else:\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n            total = 0\n            j = 0\n            for  batch_idx, (data, target) in enumerate(dataloaders[phase]):\n                data, target = Variable(data), Variable(target)\n                data = data.type(torch.cuda.FloatTensor)\n                target = target.type(torch.cuda.LongTensor)\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                _, preds = torch.max(output, 1)\n                running_corrects = running_corrects + torch.sum(preds == target.data)\n                running_loss += loss.item() * data.size(0)\n                j = j+1\n                if(phase =='train'):\n                    loss.backward()\n                    optimizer.step()\n\n                if batch_idx % 300 == 0:\n                    print('{} Epoch: {}  [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAcc: {:.6f}'.format(phase,epoch, batch_idx * len(data), len(dataloaders[phase].dataset),100. * batch_idx / len(dataloaders[phase])\n                                                                                                 , running_loss/(j*batch_size),running_corrects.double()/(j*batch_size)))\n            epoch_acc = running_corrects.double()/(len(dataloaders[phase])*batch_size)\n            epoch_loss = running_loss/(len(dataloaders[phase])*batch_size)\n            if(phase == 'val'):\n                earlystop(epoch_loss,model)\n\n            if(phase == 'train'):\n                losses.append(epoch_loss)\n                acc.append(epoch_acc)\n            print(earlystop.early_stop)\n        if(earlystop.early_stop):\n            print(\"Early stopping\")\n            model.load_state_dict(torch.load('./checkpoint.pt'))\n            break\n        print('{} Accuracy: '.format(phase),epoch_acc.item())\n    return losses,acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(dataloader):\n    running_corrects = 0\n    running_loss=0\n    pred = []\n    true = []\n    pred_wrong = []\n    true_wrong = []\n    image = []\n    sm = nn.Softmax(dim = 1)\n    for batch_idx, (data, target) in enumerate(dataloader):\n        data, target = Variable(data), Variable(target)\n        data = data.type(torch.cuda.FloatTensor)\n        target = target.type(torch.cuda.LongTensor)\n        classifier.eval()\n        output = classifier(data)\n        loss = criterion(output, target)\n        output = sm(output)\n        _, preds = torch.max(output, 1)\n        running_corrects = running_corrects + torch.sum(preds == target.data)\n        running_loss += loss.item() * data.size(0)\n        preds = preds.cpu().numpy()\n        target = target.cpu().numpy()\n        preds = np.reshape(preds,(len(preds),1))\n        target = np.reshape(target,(len(preds),1))\n        data = data.cpu().numpy()\n        \n        for i in range(len(preds)):\n            pred.append(preds[i])\n            true.append(target[i])\n            if(preds[i]!=target[i]):\n                pred_wrong.append(preds[i])\n                true_wrong.append(target[i])\n                image.append(data[i])\n      \n    epoch_acc = running_corrects.double()/(len(dataloader)*batch_size)\n    epoch_loss = running_loss/(len(dataloader)*batch_size)\n    print(epoch_acc,epoch_loss)\n    return true,pred,image,true_wrong,pred_wrong","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def error_plot(loss):\n    plt.figure(figsize=(10,5))\n    plt.plot(loss)\n    plt.title(\"Training loss plot\")\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"Loss\")\n    plt.show()\ndef acc_plot(acc):\n    plt.figure(figsize=(10,5))\n    plt.plot(acc)\n    plt.title(\"Training accuracy plot\")\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"accuracy\")\n    plt.show()\n# To plot the wrong predictions given by model\ndef wrong_plot(n_figures,true,ima,pred,encoder,inv_normalize):\n    print('Classes in order Actual and Predicted')\n    n_row = int(n_figures/3)\n    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=3)\n    for ax in axes.flatten():\n        a = random.randint(0,len(true)-1)\n    \n        image,correct,wrong = ima[a],true[a],pred[a]\n        image = torch.from_numpy(image)\n        correct = int(correct)\n        c = encoder[correct]\n        wrong = int(wrong)\n        w = encoder[wrong]\n        f = 'A:'+c + ',' +'P:'+w\n        if inv_normalize !=None:\n            image = inv_normalize(image)\n        image = image.numpy().transpose(1,2,0)\n        im = ax.imshow(image)\n        ax.set_title(f)\n        ax.axis('off')\n    plt.show()\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\ndef performance_matrix(true,pred):\n    precision = metrics.precision_score(true,pred,average='macro')\n    recall = metrics.recall_score(true,pred,average='macro')\n    accuracy = metrics.accuracy_score(true,pred)\n    f1_score = metrics.f1_score(true,pred,average='macro')\n    print('Precision: {} Recall: {}, Accuracy: {}: ,f1_score: {}'.format(precision*100,recall*100,accuracy*100,f1_score*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model,dataloaders,criterion,num_epochs=10,lr=0.0001,batch_size=8,patience = None,classes = None):\n    dataloader_train = {}\n    losses = list()\n    accuracy = list()\n    key = dataloaders.keys()\n    for phase in key:\n        if(phase == 'test'):\n            perform_test = True\n        else:\n            dataloader_train.update([(phase,dataloaders[phase])])\n    losses,accuracy = train(model,dataloader_train,criterion,num_epochs,lr,batch_size,patience)\n    error_plot(losses)\n    acc_plot(accuracy)\n    if(perform_test == True):\n        true,pred,image,true_wrong,pred_wrong = test(dataloaders['test'])\n        wrong_plot(12,true_wrong,image,pred_wrong,encoder,inv_normalize)\n        performance_matrix(true,pred)\n        if(classes !=None):\n            plot_confusion_matrix(true, pred, classes= classes,title='Confusion matrix, without normalization')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(classifier,dataloaders,criterion,50, patience = 5 , batch_size = 32 , classes = classes)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}